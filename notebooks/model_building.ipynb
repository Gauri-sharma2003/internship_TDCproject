{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate one folder back\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Change the current directory to the data folder\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/data_clean.xlsx\"\n",
    "data_clean=pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291442, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'THICKNESS', 'WIDTH', 'YS', 'UTS', 'EL', 'C', 'MN', 'S',\n",
       "       'P', 'SI', 'AL', 'N', 'TI', 'B', 'CR', 'V', 'NB', 'MO', 'CR TDC',\n",
       "       'Application_Automotive Internal',\n",
       "       'Application_Drum,Bareels,Containers', 'Application_Export',\n",
       "       'Application_Furnitures and Panels', 'Application_General Engineering',\n",
       "       'Application_Other', 'Application_Tubes', 'Application_White Goods'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Columns = ['THICKNESS', 'WIDTH', 'YS', 'UTS', 'EL', 'C', 'MN', 'S', 'P', 'SI',\n",
    "       'AL', 'N', 'TI', 'B', 'CR', 'V', 'NB', 'MO',\n",
    "       'Application_Automotive Internal',\n",
    "       'Application_Drum,Bareels,Containers', 'Application_Export',\n",
    "       'Application_Furnitures and Panels', 'Application_General Engineering',\n",
    "       'Application_Other', 'Application_Tubes', 'Application_White Goods']\n",
    "Y_Column = ['CR TDC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building without Sacling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clean[X_Columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_clean[Y_Column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = hf.split_data(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hf.train_and_evaluate_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsacling to handle imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import traceback\n",
    "\n",
    "def upscale_dataframe_with_random_oversampling(df, target_column):\n",
    "    \"\"\"\n",
    "    Upscale a DataFrame using random oversampling.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        target_column (str): Name of the target column containing class labels.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Upscaled DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Separate majority and minority classes\n",
    "        majority_class = df[df[target_column] == df[target_column].mode()[0]]\n",
    "        minority_class = df[df[target_column] != df[target_column].mode()[0]]\n",
    "        \n",
    "        # Upsample minority class\n",
    "        #if minority_class<=50:\n",
    "        minority_upsampled = resample(minority_class,\n",
    "                                      replace=True,  # Sample with replacement\n",
    "                                      n_samples=len(majority_class),  # Match majority class size\n",
    "                                      random_state=42)  # Reproducible results\n",
    "        \n",
    "        # Combine majority class with upsampled minority class\n",
    "        df_upsampled = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "        print(\"DataFrame upscaled successfully using random oversampling!\")\n",
    "        return df_upsampled\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame and 'target_column' is the name of your target column\n",
    "# df_upscaled = upscale_dataframe_with_random_oversampling(df, 'target_column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test,app):\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        with open(f\"models/{app}_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         precision = precision_score(y_test, y_pred)\n",
    "#         recall = recall_score(y_test, y_pred)\n",
    "#         f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        cr = classification_report(y_test, y_pred)\n",
    "        \n",
    "        print(\"_\"*30)\n",
    "        print(\"Model Name: \",name)\n",
    "        print(\"CLassification Report\",cr)\n",
    "        \n",
    "        results[name]=cr\n",
    "    \n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     best_model = results_df.loc[results_df['F1 Score'].idxmax()]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_cols = ['Application_Automotive Internal',\n",
    "       'Application_Drum,Bareels,Containers', 'Application_Export',\n",
    "       'Application_Furnitures and Panels', 'Application_General Engineering',\n",
    "       'Application_Other', 'Application_Tubes', 'Application_White Goods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Columns_updated = ['THICKNESS', 'WIDTH', 'YS', 'UTS', 'EL', 'C', 'MN', 'S', 'P', 'SI',\n",
    "       'AL', 'N', 'TI', 'B', 'CR', 'V', 'NB', 'MO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications_list = ['Application_Furnitures and Panels', 'Application_Automotive Internal', 'Application_Export',\n",
    "       'Application_Automotive Exposed-OEM', 'Application_White Goods', 'Application_General Engineering',\n",
    "       'Application_Tubes', 'Application_Drum,Bareels,Containers','Application_Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for app in applications_list:\n",
    "    data_AU = data_clean[data_clean[app]==1].drop(one_hot_encoded_cols,axis=1)\n",
    "    #Add code here for up-scaling\n",
    "    data_clean_us = upscale_dataframe_with_random_oversampling(data_AU,'CR TDC')\n",
    "\n",
    "    X = data_AU[X_Columns_updated]\n",
    "    y = data_AU[Y_Column]\n",
    "    X_train, X_test, y_train, y_test = hf.split_data(X,y)\n",
    "    results_AU = train_and_evaluate_models(X_train, X_test, y_train, y_test,app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_multiclass_roc_auc(y_test, y_score, n_classes):\n",
    "    \"\"\"\n",
    "    Plot ROC_AUC curve for a multiclass classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (array-like): True labels for the test set.\n",
    "    y_score (array-like): Predicted probabilities for the test set.\n",
    "    n_classes (int): Number of classes in the classification problem.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    lw = 2\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']  # You can extend this list for more classes\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve for multiclass classification')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = 'Application_Other'\n",
    "data_AU = data_clean[data_clean[app]==1].drop(one_hot_encoded_cols,axis=1)\n",
    "#Add code here for up-scaling\n",
    "data_clean_us = upscale_dataframe_with_random_oversampling(data_AU,'CR TDC')\n",
    "\n",
    "X = data_AU[X_Columns_updated]\n",
    "y = data_AU[Y_Column]\n",
    "X_train, X_test, y_train, y_test = hf.split_data(X,y)\n",
    "n_classes = y_test.shape[1]\n",
    "results_AU = train_and_evaluate_models(X_train, X_test, y_train, y_test,app,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_us = upscale_dataframe_with_random_oversampling(data_AU,'CR TDC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_AU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_AU['CR TDC'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "67*22336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_AU['CR TDC'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
